import { streamText } from 'ai';
import { openai } from '../../../lib/azure-ai.js';
import { CHATBOT_SYSTEM_PROMPT, createChatbotContext } from '../../../lib/prompts.js';
import { getComparison } from '../../../actions/comparison-actions.js';

// Allow streaming responses up to 30 seconds
export const maxDuration = 30;

export async function POST(request) {
  try {
    const { messages, comparisonId, isGeneralChat } = await request.json();

    // Validate input
    if (!messages || !Array.isArray(messages)) {
      console.error('‚ùå Invalid messages format:', typeof messages);
      return new Response('Invalid messages format', { status: 400 });
    }

    let context = '';
    let systemPrompt = CHATBOT_SYSTEM_PROMPT;

    if (isGeneralChat) {
      // General educational chatbot mode - return JSON response instead of streaming
      systemPrompt = `Tu es un assistant √©ducatif sp√©cialis√© dans le syst√®me scolaire et universitaire tunisien. Tu aides les √©tudiants avec:

- L'orientation scolaire et universitaire  
- Les informations sur les fili√®res BAC (Math, Sciences, Info, Tech, Eco, Lettres)
- Les calculs de scores et moyennes
- Les d√©bouch√©s professionnels
- Les m√©thodes d'√©tude et de r√©vision
- Les conseils pour r√©ussir ses √©tudes
- Les informations sur les universit√©s tunisiennes

R√©ponds de mani√®re claire, utile et encourageante. Utilise des exemples concrets du syst√®me √©ducatif tunisien.`;

      try {
        const response = await openai.chat.completions.create({
          model: process.env.AZURE_OPENAI_CHAT_DEPLOYMENT || 'gpt-4',
          messages: [
            { role: 'system', content: systemPrompt },
            ...messages
          ],
          max_tokens: 1000,
          temperature: 0.7,
        });

        const message = response.choices[0]?.message?.content || "D√©sol√©, je n'ai pas pu g√©n√©rer une r√©ponse.";
        
        return Response.json({ message });
      } catch (error) {
        console.error('‚ùå General chat error:', error);
        return Response.json({ 
          message: "D√©sol√©, je rencontre un probl√®me technique. Veuillez r√©essayer dans quelques instants." 
        }, { status: 500 });
      }
    } else if (comparisonId) {
      // Comparison-specific chat mode (streaming)
      console.log('üîç Loading comparison for chat...');
      
      try {
        const comparison = await getComparison(comparisonId);
        
        if (!comparison) {
          console.error('‚ùå Comparison not found:', comparisonId);
          return new Response('Comparison not found', { status: 404 });
        }

        console.log('‚úÖ Comparison loaded successfully');

        // Create context from comparison data
        context = createChatbotContext(comparison);
        console.log('üìù Context created, length:', context.length);

      } catch (error) {
        console.error('‚ùå Error loading comparison:', error);
        return new Response('Error loading comparison data', { status: 500 });
      }
    } else {
      return new Response('Either isGeneralChat or comparisonId must be provided', { status: 400 });
    }

    // For comparison chat, use streaming
    try {
      console.log('üöÄ Starting stream generation...');
      
      const result = await streamText({
        model: openai(process.env.AZURE_OPENAI_CHAT_DEPLOYMENT || 'gpt-4'),
        system: systemPrompt + (context ? `\n\nCONTEXT:\n${context}` : ''),
        messages: messages.map(msg => ({
          role: msg.role,
          content: msg.content
        })),
        maxTokens: 2000,
        temperature: 0.7,
      });

      console.log('‚úÖ Stream generation successful');
      return result.toAIStreamResponse();

    } catch (streamError) {
      console.error('‚ùå Stream generation error:', streamError);
      
      // Fallback to non-streaming response
      try {
        console.log('üîÑ Falling back to non-streaming...');
        
        const completion = await openai.chat.completions.create({
          model: process.env.AZURE_OPENAI_CHAT_DEPLOYMENT || 'gpt-4',
          messages: [
            { role: 'system', content: systemPrompt + (context ? `\n\nCONTEXT:\n${context}` : '') },
            ...messages.map(msg => ({
              role: msg.role,
              content: msg.content
            }))
          ],
          max_tokens: 2000,
          temperature: 0.7
        });

        const assistantMessage = completion.choices[0]?.message?.content || 
          "D√©sol√©, je rencontre des difficult√©s techniques. Veuillez r√©essayer.";

        return new Response(assistantMessage, {
          headers: { 'Content-Type': 'text/plain' }
        });

      } catch (fallbackError) {
        console.error('‚ùå Fallback also failed:', fallbackError);
        return new Response(
          "D√©sol√©, je rencontre des probl√®mes techniques. Veuillez r√©essayer dans quelques instants.",
          { 
            status: 500,
            headers: { 'Content-Type': 'text/plain' }
          }
        );
      }
    }

  } catch (error) {
    console.error('‚ùå Chat API Error:', error);
    
    return new Response(
      JSON.stringify({
        error: 'Une erreur s\'est produite lors du traitement de votre demande',
        details: process.env.NODE_ENV === 'development' ? error.message : undefined
      }),
      {
        status: 500,
        headers: { 'Content-Type': 'application/json' }
      }
    );
  }
}
